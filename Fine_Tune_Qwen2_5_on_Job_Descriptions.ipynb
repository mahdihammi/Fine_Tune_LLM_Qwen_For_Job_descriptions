{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU json-repair==0.29.1\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "8aev1Q7Fk1ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J6WIJkG_NszE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from os.path import join\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "svLF77ipjqxk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json_repair\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "cUxhl4edk6rf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/LLMs/datasets/combined_jobs.csv')"
      ],
      "metadata": {
        "id": "lZlZDviAQJJ5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join(os.getcwd(), 'drive/MyDrive/LLMs/datasets')"
      ],
      "metadata": {
        "id": "wJsUat9QaIM9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CY55Neflabqh",
        "outputId": "ac96cc88-fd6e-49fe-f02b-36c2f68fac9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/LLMs/datasets'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "TeXNehZkQMpf",
        "outputId": "edeec52c-d1a5-45f4-be47-9f822d3808f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   Position                Company_Name  \\\n",
              "0                                IT Manager  10 Percent Recruiting Ltd.   \n",
              "1                       Manager, IT Support                      Procom   \n",
              "2        Director of Information Technology   Southampton Financial Inc   \n",
              "3  Manager, Information Technology Services         Town of Tillsonburg   \n",
              "4                           Systems Manager                       Accor   \n",
              "\n",
              "                              Location Post_Month  Post_Year  \\\n",
              "0  Vancouver, British Columbia, Canada       June       2024   \n",
              "1             Toronto, Ontario, Canada       June       2024   \n",
              "2             Toronto, Ontario, Canada       June       2024   \n",
              "3         Tillsonburg, Ontario, Canada       June       2024   \n",
              "4           Winnipeg, Manitoba, Canada       June       2024   \n",
              "\n",
              "                                             Details  \n",
              "0  Position Title: IT Manager\\n\\nLocation: Vancou...  \n",
              "1  On behalf of our public sector client, PROCOM ...  \n",
              "2  Southampton Financials‚Äô Mission: Bring Clarity...  \n",
              "3  The Town of Tillsonburg is looking for a Manag...  \n",
              "4  Company Description\\n\\n\"Why work for Accor?\"\\n...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc095e0f-79fe-4a9d-ae59-995da60b5a7e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Position</th>\n",
              "      <th>Company_Name</th>\n",
              "      <th>Location</th>\n",
              "      <th>Post_Month</th>\n",
              "      <th>Post_Year</th>\n",
              "      <th>Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IT Manager</td>\n",
              "      <td>10 Percent Recruiting Ltd.</td>\n",
              "      <td>Vancouver, British Columbia, Canada</td>\n",
              "      <td>June</td>\n",
              "      <td>2024</td>\n",
              "      <td>Position Title: IT Manager\\n\\nLocation: Vancou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Manager, IT Support</td>\n",
              "      <td>Procom</td>\n",
              "      <td>Toronto, Ontario, Canada</td>\n",
              "      <td>June</td>\n",
              "      <td>2024</td>\n",
              "      <td>On behalf of our public sector client, PROCOM ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Director of Information Technology</td>\n",
              "      <td>Southampton Financial Inc</td>\n",
              "      <td>Toronto, Ontario, Canada</td>\n",
              "      <td>June</td>\n",
              "      <td>2024</td>\n",
              "      <td>Southampton Financials‚Äô Mission: Bring Clarity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Manager, Information Technology Services</td>\n",
              "      <td>Town of Tillsonburg</td>\n",
              "      <td>Tillsonburg, Ontario, Canada</td>\n",
              "      <td>June</td>\n",
              "      <td>2024</td>\n",
              "      <td>The Town of Tillsonburg is looking for a Manag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Systems Manager</td>\n",
              "      <td>Accor</td>\n",
              "      <td>Winnipeg, Manitoba, Canada</td>\n",
              "      <td>June</td>\n",
              "      <td>2024</td>\n",
              "      <td>Company Description\\n\\n\"Why work for Accor?\"\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc095e0f-79fe-4a9d-ae59-995da60b5a7e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc095e0f-79fe-4a9d-ae59-995da60b5a7e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc095e0f-79fe-4a9d-ae59-995da60b5a7e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7414bf20-5025-4ec6-a5b5-bb7a3df1e744\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7414bf20-5025-4ec6-a5b5-bb7a3df1e744')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7414bf20-5025-4ec6-a5b5-bb7a3df1e744 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1079,\n  \"fields\": [\n    {\n      \"column\": \"Position\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1079,\n        \"samples\": [\n          \"Data Scientist, ESG Analytics - CID&A\",\n          \"DevOps Engineer/CloudEngineer (Remote)\",\n          \"Software Engineer, Machine Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Company_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 708,\n        \"samples\": [\n          \"enableIT\",\n          \"First Media US\",\n          \"Dialpad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 177,\n        \"samples\": [\n          \"Langley, British Columbia, Canada\",\n          \"Yellowstone, Alberta, Canada\",\n          \"Deerfield, IL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Post_Month\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"February\",\n          \"June\",\n          \"October\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Post_Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 1025,\n        \"max\": 2025,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2025,\n          1025,\n          2024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Details\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1076,\n        \"samples\": [\n          \"Cresta is on a mission to turn every customer conversation into a competitive advantage by unlocking the true potential of the contact center. Our platform combines the best of AI and human intelligence to help contact centers discover customer insights and behavioral best practices, automate conversations and inefficient processes, and empower every team member to work smarter and faster. Born from the prestigious Stanford AI lab, Cresta's co-founder and chairman is Sebastian Thrun, the genius behind Google X, Waymo, Udacity, and more. Our leadership also includes CEO, Ping Wu, the co-founder of Google Contact Center AI and Vertex AI platform, and CTO & co-founder, Tim Shi, an early member of Open AI.\\n\\nJoin us on this thrilling journey to revolutionize the workforce with AI. The future of work is here, and it's at Cresta.\\n\\nAbout The Role\\n\\nWe seek driven designers to shape the future of AI Agent products at Cresta. Whether it's automating entire contact center conversations, performing tasks on behalf of agents, or coaching human agents to enhance their performance, you will play a crucial role. Your responsibilities will include designing the end-user experiences and the tools and workflows that enable these experiences.\\n\\nAs a designer at Cresta, you are at the forefront of a technological breakthrough with LLM and Generative AI. This unique position allows you to transform how people interact with AI meaningfully. You will own the end-to-end process, including product design, user research, and visual design elements, to create world-class experiences.\\n\\nResponsibilities\\n\\nYou\\u2019ll generate novel UX ideas for AI Agents, considering the needs of various user personas.\\nYou\\u2019ll think through user and business problems, propose reasonable solutions, create detailed mockups, prototype to explore further and work with engineers to build them.\\nYou're actively involved in defining product strategy and feature requirements.\\nYou'll constantly talk to our customers from contact centers - gathering insights into user pain points before and during the development process, and validating features to improve usability.\\nYou\\u2019ll collaborate closely with Engineers, PMs, ML experts, Customer Success, and Sales.\\nYou\\u2019ll be the design advocate and push for the highest quality of the polish.\\nYou\\u2019ll constantly identify areas of opportunities and future work for the products.\\nYou\\u2019ll have good judgment of when to ship: perfect is the enemy of good.\\n\\nQualifications We Value\\n\\nBachelor's degree or equivalent in UX Design, HCI, or a related field.\\nExperience with designing AI Agents, Virtual Agents, or Conversational AI.\\nExperience with enterprise SaaS customers, contact centers, and data visualization.\\nStrong portfolio showcasing elegant design solutions for complex SaaS applications.\\nProficiency in design tools such as Figma, with the ability to create interactive prototypes. Front-end development experience is a bonus.\\nStrong at product thinking + a blend of UX disciplines (interaction design, prototyping, visual design, and user research).\\nHave a deep curiosity about the underlying tech of our products.\\n\\nPerks & Benefits:\\n\\nWe offer Cresta employees a variety of medical, dental, and vision plans, designed to fit you and your family\\u2019s needs\\nPaid parental leave to support you and your family\\nMonthly Health & Wellness allowance\\nWork from home office stipend to help you succeed in a remote environment\\nLunch reimbursement for in-office employees\\nPTO: 3 weeks in Canada\\n\\nCompensation for this position includes a base salary, equity, and a variety of benefits. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. We are actively hiring for this role in the US and Canada. Your recruiter can provide further details.\\n\\nWe have noticed a rise in recruiting impersonations across the industry, where scammers attempt to access candidates' personal and financial information through fake interviews and offers. All Cresta recruiting email communications will always come from the @cresta.ai domain. Any outreach claiming to be from Cresta via other sources should be ignored. If you are uncertain whether you have been contacted by an official Cresta employee, reach out to recruiting@cresta.ai\",\n          \"As an Intermediate DevOps contributor at Martello, you will participate in the Cloud Modernization, CI/CD operationalization of Kubernetes hosted applications with consideration of industry standards and best practices and cost effectiveness and emerging technologies while working closely with multiple cross-functional teams (R&D, Product Owners, Customer Support, etc.).\\n\\nYour responsibilities will include:\\nStrong analytical skills to interpret and translate product requirements and acceptance criteria into a technical specification and then translate the resulting design into code.\\nAutomating deployments, infrastructure provisioning and management, system administration, configuration management in a cloud environment\\nMonitoring of performance and stability and incident response and disaster recovery.\\nAssessing requirements to ensure software deliverables remain secure, reliable, scalable and cost-effective\\n\\nDuring the selection process, you will be required to demonstrate or provide evidence of experience:\\nKubernetes hosted applications and infrastructures within Azure\\nImplement deployment and orchestration of artifacts within a CI/CD framework\\nExperience at producing deployment templates/recipes that automate infrastructure provisioning and deployment. g. YAML based pipelines , terraform, etc.\\n\\nDesired Skills and Experience\\nBachelor's degree in Computer Science or other technical discipline, or equivalent experience\\nMinimum 2+ years Kubernetes\\nMinimum 2+ years scripting experience in Linux CLI and Linux security\\nMinimum 2 years of experience with Terraform/Jenkins/Git or equivalent build orchestration tools\\nMinimum 2 years of experience with Azure or other virtualization infrastructure with accompanying certifications\\n2+ years of building Cloud (SaaS) products and infrastructures\\nExperience with Observability, APM, SLAs, operational visualization (Prometheus, Grafana)\\nProven experience with Linux and Windows system administration, networking administration, configuration and troubleshooting, ideally in a cloud environment\\nAgile project management tools: SCRUM / Kanban / Atlassian Confluence & Jira\\nCloud Native software architecture\\nNGINX, Web Application Firewall, OWASP, Zero trust\\nCI/CD Automation / Azure DevOps / Jenkins / Ansible / Terraform / Kubernetes / Docker / Terraform\\nPublic clouds: Azure (Primary) / Amazon (Secondary)\\nMicrosoft SQL / Postgre SQL / PowerBI / ElasticSearch / Kibana\\n\\n\\nWhat We Offer\\nA challenging position in a rapidly evolving industry.\\nA competitive base salary + commission with excellent growth and development opportunities.\\nHybrid work environment (in office a minimum of 2-3 days per week).\\nEmployer provided health insurance that starts the first day of employment.\\nMartello stock option grants.\\nGenerous paid vacation + 10 paid personal days + 2 paid days for volunteering.\\nProfessional development opportunities\\n\\nAbout Martello Technologies\\nMartello Technologies is headquartered in Ottawa, Canada with staff in Canada, Europe, and the United States. We provide Microsoft digital experience monitoring (DEM) solutions that monitor the performance of cloud collaboration and productivity tools to give enterprises insight into the user experience. Our products include unified communications (UC) performance analytics software, Microsoft 365 user experience monitoring software, and IT analytics software.\\n\\nMartello offers a positive, diverse, and supportive culture. We welcome individuals who are curious, inventive, and want to work with people who are smart, humble, hardworking, and, above all, collaborative. Martello Technologies is committed to supporting a culture of inclusion, diversity, and accessibility to employment for all. We are proud to operate as an equal opportunity employer.\\n\\nDid we spark your interest? Get in touch, we\\u2019re keen to tell you more!\",\n          \"Reddit is a community of communities. It\\u2019s built on shared interests, passion, and trust and is home to the most open and authentic conversations on the internet. Every day, Reddit users submit, vote, and comment on the topics they care most about. With 100,000+ active communities and approximately 101M+ daily active unique visitors, Reddit is one of the internet\\u2019s largest sources of information. For more information, visit redditinc.com.\\n\\nLocation:\\n\\nThis role is completely remote-friendly. If you happen to live close to one of our physical office locations, our doors are open for you to come into the office as often as you'd like.\\n\\nTeam Description\\n\\nThe Search & Recommendation Relevance team focuses on delivering the most relevant results and recommendations when users search for anything on Reddit. Our systems and algorithms operate on the world's largest corpus of human conversation, showcasing the best answers and opinions from all across Reddit on any topics, empowering discovery.\\n\\nWe are looking for Machine Learning Engineers across multiple levels to shape the future of Search products at Reddit.\\n\\nRole Description\\n\\nIn this role, you will contribute to advancing our current search systems as well as building and iterating on Reddit Answers, our next generation AI-driven search product. As a Machine Learning Engineer, you will:\\n\\nDevelop and enhance Search Retrievals and Ranking models - from optimizing lexical search (e.g. SOLR tuning) to designing and iterating on semantic models and ranking systems.\\nDesign and build pipelines and algorithms that make it effortless for users to find high-quality answers - whether it's recommendations for the best hiking trail, travel advice, or reviews of the next product or restaurant.\\nCollaborate with product managers, data scientists, ML modelers and platform engineers to build a state of the search recommender system.\\nDevelop and test new components in our pipelines, deploying ML models, integrating LLMs, and ensuring effective monitoring and product integration.\\nLeverage your technical expertise to ensure our pipelines maintain high uptime and low latency, while collaborating with other technical leaders to develop a long-term roadmap that aligns with the needs of a constantly evolving search product ecosystem.\\nThis is a high-impact role where you will be involved in technical & product strategy, operations, architecture, and execution for one of the largest sites in the world.\\n\\n\\nRequired Qualifications\\n\\n3-10+ years of industry experience as a machine learning engineer or software engineer developing backend / infrastructure at scale.\\nExperience in working and building machine learning models using PyTorch or Tensorflow.\\nExperience working with search & recommender systems and pipelines.\\nExperience building production-quality code incorporating testing, evaluation, and monitoring using object-oriented programming, including experience in Python, Golang.\\nExperienced with GraphQL, REST, HTTP, Thrift or gRPC basics, and the ability to design and implement maintainable APIs. Deep systems level understanding of industry scale recommendation systems.\\nExperience of developing applications using large scale data stack - e.g. Kubeflow, Airflow, BigQuery, GraphQL, Kafka, Redis etc.\\n\\n\\nBenefits\\n\\nComprehensive Healthcare Benefits\\n401k Matching\\nWorkspace benefits for your home office\\nPersonal & Professional development funds\\nFamily Planning Support\\nFlexible Vacation (please use them!) & Reddit Global Wellness Days\\n4+ months paid Parental Leave\\nPaid Volunteer time off\\n\\n\\nPay Transparency\\n\\nThis job posting may span more than one career level.\\n\\nIn addition to base salary, this job is eligible to receive equity in the form of restricted stock units, and depending on the position offered, it may also be eligible to receive a commission. Additionally, Reddit offers a wide range of benefits to U.S.-based employees, including medical, dental, and vision insurance, 401(k) program with employer match, generous time off for vacation, and parental leave. To learn more, please visit https://www.redditinc.com/careers/.\\n\\nTo provide greater transparency to candidates, we share base pay ranges for all US-based job postings regardless of state. We set standard base pay ranges for all roles based on function, level, and country location, benchmarked against similar stage growth companies. Final offer amounts are determined by multiple factors including, skills, depth of work experience and relevant licenses/credentials, and may vary from the amounts listed below.\\n\\nThe Base Pay Range For This Position Is\\n\\n$185,800\\u2014$322,000 USD\\n\\nReddit is proud to be an equal opportunity employer, and is committed to building a workforce representative of the diverse communities we serve. Reddit is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, please contact us at ApplicationAssistance@Reddit.com.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fD_M7wORTs_",
        "outputId": "95274d35-38ef-40a2-9395-79bcb7a35010"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Position', 'Company_Name', 'Location', 'Post_Month', 'Post_Year',\n",
              "       'Details'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = df['Details'][0]"
      ],
      "metadata": {
        "id": "jx4zSQKQRgEx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(300)"
      ],
      "metadata": {
        "id": "LkKbmxh4QM7H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR85uxFWQrud",
        "outputId": "2acd4a6e-e13a-4d1c-da28-1bc99a7b9242"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeVFJ1U9QtRM",
        "outputId": "b8f92835-aaf0-48ec-a36b-d0a1eae93c81"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Output template"
      ],
      "metadata": {
        "id": "b1JHV2PfS-QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"role_summary\": \"A concise, non-technical summary of the job role. It should describe the primary responsibilities of the role in simple language, avoiding jargon. Focus on what the role does rather than listing requirements.\",\n",
        "  \"key_terms\": [\n",
        "    {\n",
        "      \"term\": \"Technical Term or jargon from the job description\",\n",
        "      \"explanation\": \"A simple explanation of the term in the context of the role. This helps recruiters understand technical jargon without needing domain expertise.\"\n",
        "    }\n",
        "  ],\n",
        "  \"skill_priorities\": {\n",
        "    \"must_have\": [\"List of essential skills required for the role. These are non-negotiable and should be explicitly mentioned in the job description.\"],\n",
        "    \"nice_to_have\": [\"List of preferred skills that are beneficial but not mandatory. These are often marked as 'preferred,' 'a plus,' or 'optional' in the job description.\"]\n",
        "  },\n",
        "  \"proposed_screening_questions_with_answers\": [\n",
        "    {\n",
        "      \"question\": \"A role-specific question to assess candidate expertise. The question should focus on technical skills, problem-solving, or past experiences relevant to the role.\",\n",
        "      \"example_answer\": \"An example of a strong candidate response to the question. The answer should demonstrate technical depth, problem-solving, and relevance to the role.\"\n",
        "    }\n",
        "  ],\n",
        "  \"red_flags\": [\n",
        "    \"Indicators of potential mismatches for the role, offering actionable insights for recruiters based on the job offer. Examples include 'Avoid candidates without cloud experience'.\"\n",
        "  ],\n",
        "  \"confidence_score\": \"A numerical score between 0 and 100 indicating the model's confidence in the accuracy of its analysis.\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "mkquQEuaTs7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85a312e-413a-4ee3-e62b-ecab8b76f68a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role_summary': 'A concise, non-technical summary of the job role. It should describe the primary responsibilities of the role in simple language, avoiding jargon. Focus on what the role does rather than listing requirements.',\n",
              " 'key_terms': [{'term': 'Technical Term or jargon from the job description',\n",
              "   'explanation': 'A simple explanation of the term in the context of the role. This helps recruiters understand technical jargon without needing domain expertise.'}],\n",
              " 'skill_priorities': {'must_have': ['List of essential skills required for the role. These are non-negotiable and should be explicitly mentioned in the job description.'],\n",
              "  'nice_to_have': [\"List of preferred skills that are beneficial but not mandatory. These are often marked as 'preferred,' 'a plus,' or 'optional' in the job description.\"]},\n",
              " 'proposed_screening_questions_with_answers': [{'question': 'A role-specific question to assess candidate expertise. The question should focus on technical skills, problem-solving, or past experiences relevant to the role.',\n",
              "   'example_answer': 'An example of a strong candidate response to the question. The answer should demonstrate technical depth, problem-solving, and relevance to the role.'}],\n",
              " 'red_flags': [\"Indicators of potential mismatches for the role, offering actionable insights for recruiters based on the job offer. Examples include 'Avoid candidates without cloud experience'.\"],\n",
              " 'confidence_score': \"A numerical score between 0 and 100 indicating the model's confidence in the accuracy of its analysis.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "\n",
        "class key_terms(BaseModel):\n",
        "  term : str = Field(..., description=\"Technical Term or jargon from the job description\")\n",
        "  explanation : str = Field(..., description=\"A simple explanation of the term in the context of the role. This helps recruiters understand technical jargon without needing domain expertise.\")\n",
        "\n",
        "class skill_priorities(BaseModel):\n",
        "  must_have : List[str] = Field(..., description=\"List of essential skills required for the role. These are non-negotiable and should be explicitly mentioned in the job description.\")\n",
        "  nice_to_have : List[str] = Field(..., description=\"List of preferred skills that are beneficial but not mandatory. These are often marked as 'preferred,' 'a plus,' or 'optional' in the job description.\")\n",
        "\n",
        "class proposed_screening_questions_with_answers(BaseModel):\n",
        "  question : str = Field(..., description=\"A role-specific question to assess candidate expertise. The question should focus on technical skills, problem-solving, or past experiences relevant to the role.\")\n",
        "  example_answer : str = Field(..., description=\"An example of a strong candidate response to the question. The answer should demonstrate technical depth, problem-solving, and relevance to the role.\")\n",
        "\n",
        "\n",
        "class JobAnalysis(BaseModel):\n",
        "  role_summary : str\n",
        "  key_terms : List[key_terms]\n",
        "  skill_priorities : skill_priorities\n",
        "  proposed_screening_questions_with_answers : List[proposed_screening_questions_with_answers]\n",
        "  red_flags : List[str]\n",
        "  confidence_score : float\n",
        "\n"
      ],
      "metadata": {
        "id": "lSEJsj_xTAp0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## design of the extraction prompts"
      ],
      "metadata": {
        "id": "ku3Y6T-7Ru0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_messages = [\n",
        "    {\n",
        "        \"role\" : \"system\",\n",
        "        \"content\":\"\\n\".join([\n",
        "            \"You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\",\n",
        "            \"Your task is to extract key details from a given job description and format them into a JSON object with the following structure.\"\n",
        "            \"The Output Structure must follow the provided pydantic details \",\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\" : \"user\",\n",
        "        \"content\" : \"\\n\".join([\n",
        "            \"## Job Description:\",\n",
        "            example,\n",
        "            \"\"\n",
        "\n",
        "            \"## Output Structure:\",\n",
        "            json.dumps(\n",
        "                JobAnalysis.model_json_schema()\n",
        "            ),\n",
        "            \"\"\n",
        "\n",
        "\n",
        "            \"## Extraction Details: \",\n",
        "            \"\"\n",
        "\n",
        "\n",
        "        ])\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "28VwYfKfRMi1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset : Knowledge Distillation"
      ],
      "metadata": {
        "id": "624WNLlSXcKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "# Initialize client\n",
        "client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n",
        "\n",
        "# Chat completion using LLaMA 3\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gemma2-9b-it\",  # or other models like gemma-7b-it\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What's the capital of Tunisia?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the reply\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bx1xF6SWckJ",
        "outputId": "fed03598-80b6-4513-d877-73a3bded6184"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Tunisia is **Tunis**. üòÑ  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"gemma_mdoel\" : \"gemma2-9b-it\",\n",
        "    \"llama_model\" : \"llama2-7b-chat\",\n",
        "    \"mistral_model\" : \"mistral-7b-instruct\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "qFHCAHOZZILO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json(messages, model):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,  # or other models like gemma-7b-it\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Print the reply\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "JKgliCpTXuNU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = extract_json(extraction_messages, models['gemma_mdoel'])\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "VuV4NtE3Xm23",
        "outputId": "5ce1f84e-b787-4617-c1af-eb29410e9b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```json\\n{\\n  \"RoleSummary\": \"Dynamic and communicative IT Manager to join an engineering team. Requires hands-on technical expertise, vendor management mastery, exceptional stakeholder engagement, and familiarity with engineering environments.\",\\n  \"key_terms\": [\\n    {\\n      \"term\": \"MSP\",\\n      \"explanation\": \"Managed Service Provider, outsourcing IT services like network administration or helpdesk support.\"\\n    },\\n    {\\n      \"term\": \"MSSP\",\\n      \"explanation\": \"Managed Security Service Provider, specializing in cybersecurity services like threat detection and response.\"\\n    },\\n    {\\n      \"term\": \"AutoCad\",\\n      \"explanation\": \"Popular software for drafting and designing 2D and 3D drawings commonly used in engineering.\"\\n    },\\n    {\\n      \"term\": \"Rhino\",\\n      \"explanation\": \"3D modeling software widely used in engineering, product design, and manufacturing.\"\\n    }\\n  ],\\n  \"skill_priorities\": {\\n    \"must_have\": [\\n      \"Technical Expertise\",\\n      \"Vendor Management\",\\n      \"Stakeholder Engagement\",\\n      \"IT Project Management\",\\n      \"Cybersecurity\",\\n      \"Data Management\"\\n    ],\\n    \"nice_to_have\": [\\n      \"Experience with AutoCad/Rhino\",\\n      \"Information Security Regulations (North America)\"\\n    ]\\n  },\\n  \"proposed_screening_questions_with_answers\": [\\n    {\\n      \"question\": \"Describe your experience managing IT infrastructure in a fast-paced engineering environment.\",\\n      \"example_answer\": \"In my previous role at [Company Name], I oversaw the IT infrastructure for a team of 50 engineers utilizing [Specific Engineering Software]. I ensured the network was secure, scalable, and supported the demands of daily design and development workflows. I also managed contracts with third-party providers for data backups and security monitoring.\"\\n    },\\n    {\\n      \"question\": \"How do you prioritize IT projects and ensure they align with business objectives?\",\\n      \"example_answer\": \"My approach to project prioritization involves a collaborative process with stakeholders to understand their needs and business goals. I utilize a framework that considers project impact, urgency, and resource availability. Regular communication and status updates ensure alignment throughout the project lifecycle.\"\\n    }\\n  ],\\n  \"red_flags\": [\\n    \"Lack of experience in an engineering environment\",\\n    \"Limited understanding of cybersecurity best practices\",\\n    \"Poor communication or stakeholder management skills\"\\n  ],\\n  \"confidence_score\": 0.9\\n}\\n```\\n\\n\\n\\n**Explanation:**\\n\\n* **role_summary:** A concise overview of the job responsibilities and required qualifications.\\n* **key_terms:** Technical terms found in the job description, paired with simple explanations.\\n* **skill_priorities:** Categorizes essential skills (\"must-have\") and desirable skills (\"nice-to-have\") based on the job description.\\n* **proposed_screening_questions_with_answers:**  Example interview questions tailored to the role, along with model answers demonstrating desired qualities.\\n* **red_flags:**  Potential warning signs in a candidate\\'s background or responses that might indicate a poor fit for the role.\\n* **confidence_score:**  A metric (between 0 and 1) reflecting the AI\\'s confidence level in the accuracy of the extracted information. (This score is a placeholder and would be determined by the AI\\'s analysis).\\n\\n**Key Points:**\\n\\n* This JSON structure provides a standardized way to represent job descriptions, making it easier for recruiters to analyze and compare roles.\\n* The inclusion of key terms and example screening questions helps recruiters understand the technical requirements and assess candidates more effectively.\\n* Red flags can help recruiters identify potential issues early in the hiring process.  \\n\\n\\n\\nLet me know if you want me to elaborate on any specific aspect of the output or if you have another job description to analyze!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_jons_result = parse_json(result)\n",
        "\n",
        "parsed_jons_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qo4YqfyliGG",
        "outputId": "43481b04-992f-41a9-9a7a-81659acbe93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'RoleSummary': 'Dynamic and communicative IT Manager to join an engineering team. Requires hands-on technical expertise, vendor management mastery, exceptional stakeholder engagement, and familiarity with engineering environments.',\n",
              " 'key_terms': [{'term': 'MSP',\n",
              "   'explanation': 'Managed Service Provider, outsourcing IT services like network administration or helpdesk support.'},\n",
              "  {'term': 'MSSP',\n",
              "   'explanation': 'Managed Security Service Provider, specializing in cybersecurity services like threat detection and response.'},\n",
              "  {'term': 'AutoCad',\n",
              "   'explanation': 'Popular software for drafting and designing 2D and 3D drawings commonly used in engineering.'},\n",
              "  {'term': 'Rhino',\n",
              "   'explanation': '3D modeling software widely used in engineering, product design, and manufacturing.'}],\n",
              " 'skill_priorities': {'must_have': ['Technical Expertise',\n",
              "   'Vendor Management',\n",
              "   'Stakeholder Engagement',\n",
              "   'IT Project Management',\n",
              "   'Cybersecurity',\n",
              "   'Data Management'],\n",
              "  'nice_to_have': ['Experience with AutoCad/Rhino',\n",
              "   'Information Security Regulations (North America)']},\n",
              " 'proposed_screening_questions_with_answers': [{'question': 'Describe your experience managing IT infrastructure in a fast-paced engineering environment.',\n",
              "   'example_answer': 'In my previous role at [Company Name], I oversaw the IT infrastructure for a team of 50 engineers utilizing [Specific Engineering Software]. I ensured the network was secure, scalable, and supported the demands of daily design and development workflows. I also managed contracts with third-party providers for data backups and security monitoring.'},\n",
              "  {'question': 'How do you prioritize IT projects and ensure they align with business objectives?',\n",
              "   'example_answer': 'My approach to project prioritization involves a collaborative process with stakeholders to understand their needs and business goals. I utilize a framework that considers project impact, urgency, and resource availability. Regular communication and status updates ensure alignment throughout the project lifecycle.'}],\n",
              " 'red_flags': ['Lack of experience in an engineering environment',\n",
              "  'Limited understanding of cybersecurity best practices',\n",
              "  'Poor communication or stakeholder management skills'],\n",
              " 'confidence_score': 0.9}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = []\n",
        "\n",
        "for item in df['Details']:\n",
        "  raw_data.append(item)\n"
      ],
      "metadata": {
        "id": "KOHnc7M1cB5M"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from os.path import join\n",
        "\n",
        "output_path = join(data_dir, 'jobs_descriptions.jsonl')\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    for item in raw_data:\n",
        "        if item and isinstance(item, str) and item.strip():\n",
        "            json_line = json.dumps({\"description\": item.strip()})\n",
        "            f.write(json_line + \"\\n\")"
      ],
      "metadata": {
        "id": "XWGAGnJfhZ1E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path"
      ],
      "metadata": {
        "id": "JQgWuA6Qh1Xq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2dbb0296-6893-4c8c-a634-feac5a3568b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/LLMs/datasets/jobs_descriptions.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cefp34Jgocw",
        "outputId": "1a591aad-062c-4d92-f6d8-eb77aba74c3d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "new_raw_data = []\n",
        "for line in open(output_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    new_raw_data.append(\n",
        "        json.loads(line.strip())\n",
        "    )\n",
        "\n",
        "random.Random(101).shuffle(new_raw_data)\n",
        "\n",
        "print(f\"Raw data: {len(new_raw_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnkKWQEmZhm2",
        "outputId": "7e230d65-20d4-4fed-d0e9-e034184abf14"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models['gemma_mdoel']\n",
        "\n",
        "save_to = join(data_dir, \"job_descriptions\", \"job_descriptions_fine_tuning_data.jsonl\")\n",
        "\n",
        "\n",
        "for story in tqdm(new_raw_data):\n",
        "\n",
        "    sample_details_extraction_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\",\n",
        "                \"Your task is to extract key details from a given job description and format them into a JSON object with the following structure.\"\n",
        "                \"The Output Structure must follow the provided pydantic details \",\n",
        "            ])\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"## Story:\",\n",
        "                story['description'].strip(),\n",
        "                \"\",\n",
        "\n",
        "                \"## Output Structure:\",\n",
        "                json.dumps(\n",
        "                    JobAnalysis.model_json_schema(), ensure_ascii=False\n",
        "                ),\n",
        "                \"\",\n",
        "\n",
        "                \"## Extraction Details: \",\n",
        "                \"```json\"\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = extract_json(sample_details_extraction_messages, model)\n",
        "\n",
        "    llm_response = response\n",
        "    llm_resp_dict = parse_json(llm_response)\n",
        "\n",
        "    if not llm_resp_dict:\n",
        "        continue\n",
        "\n",
        "    with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"description\": story['description'].strip(),\n",
        "            \"task\": \"Extrat the job description details into a JSON.\",\n",
        "            \"output_scheme\": json.dumps(JobAnalysis.model_json_schema()),\n",
        "            \"response\": llm_resp_dict,\n",
        "                                        }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n"
      ],
      "metadata": {
        "id": "4BHZU5MFbqhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples of the distilled data created for the training using gemma2-9b-it"
      ],
      "metadata": {
        "id": "ppFrb2Ic4Nsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_data_path = os.path.join(data_dir, \"job_descriptions\", \"job_descriptions_fine_tuning_data.jsonl\")\n",
        "with open(sft_data_path, \"r\") as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "2AH8N8O0ra-R"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [json.loads(line) for line in lines]"
      ],
      "metadata": {
        "id": "mHIh3TnyxL9V"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['description']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "2kcRHILjxNvB",
        "outputId": "a494b56f-5463-4574-bfa4-b05c4981f991"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Benefits:\\n\\nBonus based on performance\\nCompetitive salary\\nHome office stipend\\nPaid time off\\nTraining & development\\n\\n\\nWe are seeking a DevOps Engineer to join our growing team and take ownership of our cloud infrastructure, CI/CD pipelines, system reliability, observability, and scalability. The ideal candidate is passionate about automation, cloud technologies, and security best practices, ensuring seamless deployment and high availability of our applications.\\n\\nResponsibilities\\n\\nDesign, implement, and manage CI/CD pipelines to streamline software development and deployment.\\nMaintain and optimize cloud infrastructure (AWS, Azure) to ensure scalability, security, and cost-effectiveness.\\nAutomate infrastructure provisioning, monitoring, and management using Infrastructure as Code (IaC) tools (Terraform, Ansible, etc.).\\nMonitor system performance, troubleshoot issues, and ensure high availability and reliability.\\nCollaborate with software engineers to enhance deployment strategies and improve development workflows.\\nImplement security best practices to safeguard infrastructure and applications.\\nManage containerization and orchestration tools like Docker and AWS ECS.\\nOptimize logging, monitoring, and alerting systems (ELK stack, etc.).\\nStay up to date with the latest DevOps trends, tools, and best practices.\\n\\n\\nRequirements\\n\\n3+ years of experience in a DevOps or Site Reliability Engineering (SRE) role.\\nStrong proficiency in cloud platforms (AWS, GCP, Azure) and cloud-native services.\\nExperience with CI/CD tools (GitHub Actions, GitLab CI/CD, etc.).\\nProficiency in Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Ansible.\\nStrong scripting skills in Bash, Python for automation.\\nHands-on experience with Docker and AWS ECS for container orchestration.\\nKnowledge of monitoring and logging tools (ELK stack, Datadog, etc.).\\nExperience with database management and performance optimization (SQL, NoSQL).\\nUnderstanding of security best practices, networking, and system administration.\\nExperience working in an agile startup environment is a plus.\\n\\n\\nNice-to-Have Skills\\n\\nDevelopment experience with Django\\nHands-on experience with Kubernetes for container orchestration.\\nExposure to AI/ML workloads.\\n\\n\\nWho we are\\n\\nRemarcable is a cloud based platform that helps electrical contractors and distributors streamline purchasing processes to save time and money.\\n\\nDedicated to the Electrical Contractor Industry, Remarcable provides cloud-based Procurement & Tool Management Software nationwide. With multiple workflows, two applications in one software, and direct contractor accounting integrations, Remarcable significantly, and efficiently, increases communication, streamlines workflows, and provides visibility for all users.\\n\\nOur team is composed of contractor and distribution experts located coast to coast. Through collaborations with industry leaders, we've gained insight into the struggles they face. Together, we believe in providing a solution that brings efficiency, visibility, and better communication to streamline the relationship between the contractor and distributor partners.\\n\\nOur Mission\\n\\nTo advance the adoption of technology in the construction industry and bring better efficiency, visibility, and communication to our customers.\\n\\nSalary Range Disclaimer\\n\\nThe base salary range represents the low and high end of the Remarcable salary range for this position. Actual salaries will vary depending on factors including but not limited to location, experience, and performance. The range listed is just one component of Remarcable‚Äôs total compensation package for employees. Other components may include: PTO and a Bonus plan.\\n\\nWork remote temporarily due to COVID-19.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gabSC5604K8x",
        "outputId": "2920942b-ed65-4e91-d3f4-f3e64035d598"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': \"Reports to: Vice President of Operations \\n About Aroris: \\n Aroris Health is a leading healthcare technology company. Our proprietary platform, Aroris360, empowers healthcare providers with data-driven insights to optimize payer negotiations, enhance operational efficiency, and improve healthcare outcomes. We are driven by innovation, committed to excellence, and focused on delivering measurable impact to our clients.  \\n Aroris's mission is to preserve the spirit of medicine. One way of doing this is to provide independent medical practices access to the same payer contracting resources as the largest health systems. We operate as an extension of practices, leveraging decades of industry experience, cutting-edge data analytics capabilities, and dedicated legal and negotiation teams to help practices capture more revenue without expending additional resources. \\n At Aroris, we look for individuals who embody our core values of being driven, joyful, relentless, and team players. We believe these qualities are vital in achieving our collective goals and fostering a thriving work environment. \\n Job Overview: \\n We are seeking a talented and detail-oriented Operations Data Analyst to join our team. As an Operations Data Analyst, your primary responsibilities will revolve around collecting, cleaning, analyzing, and sharing data as well as uploading data to our SaaS platform. You will play a crucial role in supporting decision-making processes and improving business operations through data-driven insights. Additionally, you will collaborate closely with clients and the operations team, providing clear communication and routine updates to ensure the successful execution of data-related projects. Data sources will include Excel, PDFs, EMR/Practice Management, Clearing House Revenue Reports, as well as other types of reporting software.   \\n Job Responsibilities: \\n Data Collection: Gather data from various sources, including databases, APIs, and spreadsheets, while ensuring data quality and integrity \\nData Cleaning: Perform data cleansing and transformation tasks to ensure accuracy, consistency, and completeness of the data \\nData Analysis: Utilize statistical techniques and analytical tools to analyze large datasets, identify trends, patterns, and anomalies, and extract meaningful insights that drive business decisions \\nData Sharing: Prepare and present reports, visualizations, and dashboards to communicate findings and recommendations effectively to stakeholders \\nClient and Operations Partnership: Collaborate with clients and the operations team to understand their requirements, goals, and challenges. Provide regular updates on project progress and maintain open lines of communication \\nProposal Modeling: Model healthcare reimbursement proposals from payers and share insights with negotiation teams   \\nSaaS Platform Management: Upload data into the SaaS platform, ensuring accurate and timely data entry, and troubleshooting any issues that may arise \\nData Accuracy Assurance: Implement data quality checks and validation procedures to ensure data accuracy, completeness, and consistency across all data sources \\nProcess Improvement: Continuously evaluate and enhance data collection, analysis, and reporting processes to optimize efficiency and effectiveness \\n Required Experience, Qualifications and Skills: \\n2+ years using Microsoft Excel - data collection, cleaning, standardization, mining, analysis, and visualization \\n2+ years of experience in data analysis role \\nSolid understanding of the US HealthCare reimbursement model, Fee Schedules, and Insurance Payer processes \\nDemonstrated ability to use data as a tool for problem-solving \\nExcellent communication skills, specifically as it relates to explaining technical concepts and data-driven findings \\n Preferred Experience, Qualifications and Skills: \\n2+ years of experience in database engineering \\nTableau, Power BI, or other data visualization programs \\n2+ year of experience building, maintaining, and implementing financial models \\n Compensation:\\nSalary: $55-75K\\nBonus Eligible: 15-30%\\n\\nEqual Employment Opportunity:\\u202f \\nAroris is an equal opportunity employer. Aroris is committed to equal employment opportunity in accordance with applicable federal, state, and local laws. Aroris will not discriminate against applicants for employment on any legally recognized basis. This includes, but is not limited to veteran status, race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age and physical or mental disability.\",\n",
              " 'task': 'Extrat the job description details into a JSON.',\n",
              " 'output_scheme': '{\"$defs\": {\"key_terms\": {\"properties\": {\"term\": {\"description\": \"Technical Term or jargon from the job description\", \"title\": \"Term\", \"type\": \"string\"}, \"explanation\": {\"description\": \"A simple explanation of the term in the context of the role. This helps recruiters understand technical jargon without needing domain expertise.\", \"title\": \"Explanation\", \"type\": \"string\"}}, \"required\": [\"term\", \"explanation\"], \"title\": \"key_terms\", \"type\": \"object\"}, \"proposed_screening_questions_with_answers\": {\"properties\": {\"question\": {\"description\": \"A role-specific question to assess candidate expertise. The question should focus on technical skills, problem-solving, or past experiences relevant to the role.\", \"title\": \"Question\", \"type\": \"string\"}, \"example_answer\": {\"description\": \"An example of a strong candidate response to the question. The answer should demonstrate technical depth, problem-solving, and relevance to the role.\", \"title\": \"Example Answer\", \"type\": \"string\"}}, \"required\": [\"question\", \"example_answer\"], \"title\": \"proposed_screening_questions_with_answers\", \"type\": \"object\"}, \"skill_priorities\": {\"properties\": {\"must_have\": {\"description\": \"List of essential skills required for the role. These are non-negotiable and should be explicitly mentioned in the job description.\", \"items\": {\"type\": \"string\"}, \"title\": \"Must Have\", \"type\": \"array\"}, \"nice_to_have\": {\"description\": \"List of preferred skills that are beneficial but not mandatory. These are often marked as \\'preferred,\\' \\'a plus,\\' or \\'optional\\' in the job description.\", \"items\": {\"type\": \"string\"}, \"title\": \"Nice To Have\", \"type\": \"array\"}}, \"required\": [\"must_have\", \"nice_to_have\"], \"title\": \"skill_priorities\", \"type\": \"object\"}}, \"properties\": {\"role_summary\": {\"title\": \"Role Summary\", \"type\": \"string\"}, \"key_terms\": {\"items\": {\"$ref\": \"#/$defs/key_terms\"}, \"title\": \"Key Terms\", \"type\": \"array\"}, \"skill_priorities\": {\"$ref\": \"#/$defs/skill_priorities\"}, \"proposed_screening_questions_with_answers\": {\"items\": {\"$ref\": \"#/$defs/proposed_screening_questions_with_answers\"}, \"title\": \"Proposed Screening Questions With Answers\", \"type\": \"array\"}, \"red_flags\": {\"items\": {\"type\": \"string\"}, \"title\": \"Red Flags\", \"type\": \"array\"}, \"confidence_score\": {\"title\": \"Confidence Score\", \"type\": \"number\"}}, \"required\": [\"role_summary\", \"key_terms\", \"skill_priorities\", \"proposed_screening_questions_with_answers\", \"red_flags\", \"confidence_score\"], \"title\": \"JobAnalysis\", \"type\": \"object\"}',\n",
              " 'response': {'JobAnalysis': {'role_summary': 'Operations Data Analyst will collect, clean, analyze, and share data.  You will play a crucial role in supporting decision-making processes and improving business operations through data-driven insights.',\n",
              "   'key_terms': [{'term': 'Compensation',\n",
              "     'explanation': 'Refers to the salary and bonus structure offered for this role.'},\n",
              "    {'term': 'SaaS Platform',\n",
              "     'explanation': 'A Software as a Service platform, in this case, Aroris360, which provides healthcare technology solutions.'},\n",
              "    {'term': 'EMR/Practice Management',\n",
              "     'explanation': 'Electronic Medical Records and Practice Management systems used in healthcare to store patient information and manage operations.'},\n",
              "    {'term': 'Fee Schedules',\n",
              "     'explanation': 'Standard pricing structures used by insurance payers for different medical procedures and services.'}],\n",
              "   'skill_priorities': {'must_have': ['Microsoft Excel',\n",
              "     'Data Analysis',\n",
              "     'Understanding of US Healthcare Reimbursement Model'],\n",
              "    'nice_to_have': ['Database Engineering',\n",
              "     'Tableau/Power BI',\n",
              "     'Financial Modeling']},\n",
              "   'proposed_screening_questions_with_answers': [{'question': 'Describe your experience working with large datasets in Microsoft Excel. What specific techniques did you use for data cleaning, analysis, and visualization?',\n",
              "     'example_answer': 'In my previous role, I regularly used Excel to analyze patient billing data. I utilized functions like VLOOKUP, SUMIFS, and Pivot Tables to identify trends and patterns. For data cleaning, I employed techniques like removing duplicates, validating formats, and handling missing values.'},\n",
              "    {'question': \"Explain the concept of 'Fee Schedules' in the context of the US healthcare system. How do they impact provider reimbursement?\",\n",
              "     'example_answer': 'Fee schedules are predetermined pricing lists set by insurance payers for specific medical procedures. They outline how much a payer will cover for each service. Providers submit claims to payers who then reimburse them based on the corresponding fee schedule rates, potentially negotiating different rates for bulk agreements or specific conditions.'}],\n",
              "   'red_flags': ['Lack of understanding of US healthcare reimbursement models',\n",
              "    'Limited experience with Microsoft Excel beyond basic functions',\n",
              "    'Difficulty articulating data-driven insights or problem-solving approaches'],\n",
              "   'confidence_score': 0.9}}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning using Unsloth"
      ],
      "metadata": {
        "id": "ubExCNto1DhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\""
      ],
      "metadata": {
        "id": "myRnuDzjs8yB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for fine tune"
      ],
      "metadata": {
        "id": "DW4Zl_eE0_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "sft_data_path = os.path.join(data_dir, \"job_descriptions\", \"job_descriptions_fine_tuning_data.jsonl\")\n",
        "with open(sft_data_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"The Output Structure must follow the provided pydantic details.\"\n",
        "])\n",
        "\n",
        "# Prepare and shuffle data\n",
        "llm_finetuning_data = []\n",
        "\n",
        "for line in lines:\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    instruction = \"\\n\".join([\n",
        "        system_message,\n",
        "        \"\",\n",
        "        \"# description:\",\n",
        "        rec[\"description\"],\n",
        "        \"\",\n",
        "        \"# Task:\",\n",
        "        rec[\"task\"],\n",
        "        \"\",\n",
        "        \"# Output Scheme:\",\n",
        "        rec[\"output_scheme\"],\n",
        "        \"\",\n",
        "        \"# Output JSON:\"\n",
        "\n",
        "    ])\n",
        "\n",
        "    output = json.dumps(rec[\"response\"], ensure_ascii=False, indent=2)\n",
        "\n",
        "    llm_finetuning_data.append({\n",
        "        \"instruction\": instruction,\n",
        "        \"input\": \"\",\n",
        "        \"output\": output\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetuning_data)\n",
        "\n",
        "# Convert to Hugging Face dataset\n",
        "dataset = Dataset.from_list(llm_finetuning_data)\n",
        "\n",
        "# Prepare for Unsloth fine-tuning with Alpaca-style prompt\n"
      ],
      "metadata": {
        "id": "2_BLzje8xscN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zofxUgK24lyp",
        "outputId": "e133acbd-33ba-418d-bd43-1c1ef91c2018"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output'],\n",
              "    num_rows: 112\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOZBqZG043-5",
        "outputId": "d2bc7df9-7398-4075-b1c5-84d9efa69b65"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\\nFollow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\\nThe Output Structure must follow the provided pydantic details.\\n\\n# description:\\nCompany Description\\n\\nAt Sleep Country Canada/Dormez-vous? (SCC/DV), we are inspired every day through our purpose to transform lives by awakening Canadians to the power of sleep and our vision to champion sleep as the key to healthier and happier lives, helping everyone achieve better tomorrows through better tonight‚Äôs.\\n\\nGuided by our values ‚Äì We CARE About People; We WIN Together; We DREAM Big and We DELIVER with Excellence ‚Äì we are building on our 30-year foundation of taking care of each other and our customers‚Äô sleep needs, with passion and commitment to be the best that we can be. We invest in our sleep ecosystem, innovative products, world-class customer experience, our communities and diverse best-in-class team to be Canada‚Äôs leading sleep partner.\\n\\nJob Description\\n\\nThe Senior IT Security Analyst ensures that all in-scope day to day, and project activities are properly defined; effectively managed; deliver the expected results; and meet SCC standards and policies, and that documentation, deployment, and testing is performed according to professional industry standards.\\n\\nReporting to the Manager, Information Security, responsibilities include but are not limited to;\\n\\nConduct studies that evaluate, recommend, and implement security solutions to enhance core security capabilities in the areas of security infrastructure, access management, identity management, networking, databases, servers.\\nPerform the deployment, integration, and initial configuration of all new security solutions and enhancements to existing security solutions in accordance with standards and best practices.\\nResearch and provide gap analysis of the current processes leading to the completion of documenting current processes and identifying opportunities for process improvements.\\nEvaluate internal and external environment for threats, changes, related to Information Security and perform the role as Information Security subject matter expert to ensure these are properly addressed and controlled.\\nWork with cross-functional teams to develop and implement incident response plans, including documenting procedures and conducting training exercises.\\nManage and maintain security technologies such as firewalls, intrusion detection and prevention systems, and security information and event management (SIEM) tools.\\nAssess information risk and facilitate remediation of identified vulnerabilities for IT security across the enterprise;\\nResolve security incidents in a timely and effective manner, ensuring minimal impact to the organization and learning from incidents to prevent future occurrences.\\nLead and participate in the design and execution of vulnerability assessments, penetration tests and security audits.\\nOngoing management of the organization‚Äôs security awareness program; ensure that organizational processes adhere to regulatory compliance requirements.\\nConduct research on emerging security threats and trends, and develop strategies to mitigate risks\\nProvide reporting and data-driven insights on the organization‚Äôs security posture, including vulnerabilities, incidents, and remediation efforts to senior management.\\n\\nQualifications\\n\\n8+ years of work experience in IT Security or equivalent combination of transferrable experience and education through university or college degree in an IT related field.\\nProven leadership abilities including effective knowledge sharing, conflict resolution, facilitation of open discussions, fairness and displaying appropriate levels of assertiveness.\\nProven ability to work under stress in emergencies with flexibility to handle multiple high-pressure situations simultaneously.\\nThorough knowledge of Information security principles and framework (ISO 27001, NIST, ZTNA, etc..).\\nThorough knowledge and experience on securing on-perm infrastructure, Google Cloud and Azure.\\nThorough knowledge and experience on Microsoft security tools and processes.\\nThorough knowledge on firewalls (Palo Alto), DNS, Cloudflare, Switches, Citrix, etc.\\nAbility to secure online and on-premise infrastructures, filter out suspicious activity, and find and mitigate security risks before any breaches can occur.\\nHands-on experience in security incident investigation and resolution.\\nCyber and Technical Threat Analyses\\nAbility to communicate highly complex technical information clearly and articulately for all levels and audiences.\\nAbility to manage tasks independently and take ownership of responsibilities\\nStrong customer focus with ability to manage customer expectations and experience and build long-term relationships.\\nStrong team-oriented interpersonal skills with the ability to interface with a broad range of people and roles including vendors and IT-business personnel.\\nAbility to adapt to a rapidly changing environment\\nHigh critical thinking skills to evaluate alternatives and present solutions that are consistent with business objectives and strategy.\\nThorough knowledge of patching and deployment technologies for windows platforms\\nStrong technical knowledge of current systems, software, protocols and standards. Including TCP/IP and network administration/protocols\\nExperience developing, documenting and maintaining procedures.\\nAbility to learn from mistakes and apply constructive feedback to improve performance.\\nAny one or more security certifications (CISSP, CISA, CEH, GIAC, SANS).\\n\\nAdditional Information\\n\\nWhy members of our Corporate team love working at Sleep Country Canada/Dormez-vous?:\\n\\nThis is not a job but a CAREER with opportunities for growth and advancement\\nDiverse and inclusive work environment\\nWe will invest in you and provide extensive training, mentoring and continuous development\\nAccess to training and development platforms\\nFull medical, dental benefits and a Deferred Profit Sharing Program\\nAnnual Wellness Credit of up to $250.00 for any products/services that improve your health and well-being, i.e., health assessments, nutrition counselling, hiking shoes, a yoga outfit or fitness equipment!\\nAssociate Discount Program where you will be able to enjoy some of the world‚Äôs best sleep products\\nMaternity/Parental leave top up benefits\\nTuition Reimbursement Program that covers professional AND personal development\\nLong service awards, celebrations and other social events\\nAssociate Referral Program\\nPaid day off to volunteer at your local charity of choice\\nRecognized as one of Canada‚Äôs Most Admired Corporate Cultures in 2023 by Waterstone Human Capital\\n\\nCommitment to Equity, Diversity, Inclusion & Belonging (EDI&B)\\n\\nAt SCC/DV, we are committed to building a company culture of inclusion and diversity where differences are embraced and valued, this allows us to better understand and meet the needs of our customers and the communities we serve. We want to ensure every job applicant is treated fairly and with respect regarding race, national or ethnic origin, religion, age, gender, sexual orientation, or disability.\\n\\nAbout Sleep Country Canada/Dormez-vous?\\n\\nSleep Country is Canada‚Äôs leading specialty sleep retailer with a purpose to transform lives by awakening Canadians to the power of sleep. Sleep Country Canada operates under the retailer banners; Sleep Country, Dormez-vous, the rest, Endy, Hush, Silk & Snow and most recently acquired, Casper Canada. The Company has omnichannel and ecommerce operations including over 300 corporate-owned stores and 18 distribution centers warehouses across Canada. Recognized as one of Canada‚Äôs Most Admired Corporate Cultures in 2023 by Waterstone Human Capital, Sleep Country is committed to building a company culture of inclusion and diversity where differences are embraced and valued. The Company actively invests in its sleep ecosystem, innovative products, world-class customer experience, communities and its people. For more information about Sleep Country, please visit www.sleepcountry.ca.\\n\\n# Task:\\nExtrat the job description details into a JSON.\\n\\n# Output Scheme:\\n{\"$defs\": {\"key_terms\": {\"properties\": {\"term\": {\"description\": \"Technical Term or jargon from the job description\", \"title\": \"Term\", \"type\": \"string\"}, \"explanation\": {\"description\": \"A simple explanation of the term in the context of the role. This helps recruiters understand technical jargon without needing domain expertise.\", \"title\": \"Explanation\", \"type\": \"string\"}}, \"required\": [\"term\", \"explanation\"], \"title\": \"key_terms\", \"type\": \"object\"}, \"proposed_screening_questions_with_answers\": {\"properties\": {\"question\": {\"description\": \"A role-specific question to assess candidate expertise. The question should focus on technical skills, problem-solving, or past experiences relevant to the role.\", \"title\": \"Question\", \"type\": \"string\"}, \"example_answer\": {\"description\": \"An example of a strong candidate response to the question. The answer should demonstrate technical depth, problem-solving, and relevance to the role.\", \"title\": \"Example Answer\", \"type\": \"string\"}}, \"required\": [\"question\", \"example_answer\"], \"title\": \"proposed_screening_questions_with_answers\", \"type\": \"object\"}, \"skill_priorities\": {\"properties\": {\"must_have\": {\"description\": \"List of essential skills required for the role. These are non-negotiable and should be explicitly mentioned in the job description.\", \"items\": {\"type\": \"string\"}, \"title\": \"Must Have\", \"type\": \"array\"}, \"nice_to_have\": {\"description\": \"List of preferred skills that are beneficial but not mandatory. These are often marked as \\'preferred,\\' \\'a plus,\\' or \\'optional\\' in the job description.\", \"items\": {\"type\": \"string\"}, \"title\": \"Nice To Have\", \"type\": \"array\"}}, \"required\": [\"must_have\", \"nice_to_have\"], \"title\": \"skill_priorities\", \"type\": \"object\"}}, \"properties\": {\"role_summary\": {\"title\": \"Role Summary\", \"type\": \"string\"}, \"key_terms\": {\"items\": {\"$ref\": \"#/$defs/key_terms\"}, \"title\": \"Key Terms\", \"type\": \"array\"}, \"skill_priorities\": {\"$ref\": \"#/$defs/skill_priorities\"}, \"proposed_screening_questions_with_answers\": {\"items\": {\"$ref\": \"#/$defs/proposed_screening_questions_with_answers\"}, \"title\": \"Proposed Screening Questions With Answers\", \"type\": \"array\"}, \"red_flags\": {\"items\": {\"type\": \"string\"}, \"title\": \"Red Flags\", \"type\": \"array\"}, \"confidence_score\": {\"title\": \"Confidence Score\", \"type\": \"number\"}}, \"required\": [\"role_summary\", \"key_terms\", \"skill_priorities\", \"proposed_screening_questions_with_answers\", \"red_flags\", \"confidence_score\"], \"title\": \"JobAnalysis\", \"type\": \"object\"}\\n\\n# Output JSON:', 'input': '', 'output': '{\\n  \"JobAnalysis\": {\\n    \"role_summary\": \"The Senior IT Security Analyst ensures that all in-scope day to day, and project activities are properly defined; effectively managed; deliver the expected results; and meet SCC standards and policies, and that documentation, deployment, and testing is performed according to professional industry standards.\",\\n    \"key_terms\": [\\n      {\\n        \"term\": \"ISO 27001\",\\n        \"explanation\": \"A widely recognized international standard for information security management systems (ISMS). It provides a framework for establishing, implementing, maintaining, and continually improving an ISMS.\"\\n      },\\n      {\\n        \"term\": \"NIST\",\\n        \"explanation\": \"The National Institute of Standards and Technology (NIST), a U.S. government agency, develops and promotes cybersecurity standards and guidelines.\"\\n      },\\n      {\\n        \"term\": \"ZTNA\",\\n        \"explanation\": \"Zero Trust Network Access (ZTNA) is a security framework that assumes no inherent trust, requiring continuous verification for every user, device, and network request.\"\\n      },\\n      {\\n        \"term\": \"SIEM\",\\n        \"explanation\": \"Security Information and Event Management (SIEM) tools collect and analyze security data from various sources, providing real-time insights and alerting on potential threats.\"\\n      },\\n      {\\n        \"term\": \"Firewall\",\\n        \"explanation\": \"A network security system that monitors and controls incoming and outgoing network traffic based on predefined security rules.\"\\n      },\\n      {\\n        \"term\": \"Penetration Test\",\\n        \"explanation\": \"A simulated cyberattack conducted to identify vulnerabilities in a system or network.\"\\n      },\\n      {\\n        \"term\": \"Vulnerability Assessment\",\\n        \"explanation\": \"A process of identifying and evaluating potential weaknesses in a system or network that could be exploited by attackers.\"\\n      },\\n      {\\n        \"term\": \"CISSP\",\\n        \"explanation\": \"Certified Information Systems Security Professional, a globally recognized cybersecurity certification.\"\\n      },\\n      {\\n        \"term\": \"CISA\",\\n        \"explanation\": \"Certified Information Systems Auditor, a certification focused on IT auditing and control.\"\\n      },\\n      {\\n        \"term\": \"CEH\",\\n        \"explanation\": \"Certified Ethical Hacker, a certification focused on penetration testing and ethical hacking techniques.\"\\n      },\\n      {\\n        \"term\": \"GIAC\",\\n        \"explanation\": \"Global Information Assurance Certification, a vendor-neutral cybersecurity certification consortium.\"\\n      },\\n      {\\n        \"term\": \"SANS\",\\n        \"explanation\": \"SANS Institute, a leading cybersecurity training and certification provider.\"\\n      }\\n    ],\\n    \"skill_priorities\": {\\n      \"must_have\": [\\n        \"Thorough knowledge of information security principles\",\\n        \"Experience in securing on-premises and cloud infrastructure (Google Cloud, Azure)\",\\n        \"Experience with Microsoft security tools and processes\",\\n        \"Knowledge of firewalls (Palo Alto), DNS, Cloudflare, Switches, Citrix\",\\n        \"Ability to resolve security incidents\",\\n        \"Strong communication and interpersonal skills\"\\n      ],\\n      \"nice_to_have\": [\\n        \"Experience in vulnerability assessments and penetration testing\",\\n        \"One or more security certifications (CISSP, CISA, CEH, GIAC, SANS)\"\\n      ]\\n    },\\n    \"proposed_screening_questions_with_answers\": [\\n      {\\n        \"question\": \"Describe your experience in securing on-premises and cloud infrastructure (specifically Google Cloud and Azure).\",\\n        \"example_answer\": \"I have over 6 years of experience securing both on-premises and cloud infrastructure. For Google Cloud, I\\'ve implemented security best practices for network security, identity management, data protection, and threat detection. In Azure, I\\'ve worked with Azure Active Directory, Azure Security Center, and Azure Firewall to build secure and compliant environments.\"\\n      },\\n      {\\n        \"question\": \"Walk me through your approach to resolving a complex security incident.\",\\n        \"example_answer\": \"My approach involves quickly analyzing the situation to determine the scope and severity of the incident. I would then gather relevant information, such as logs, network traffic, and affected systems. Based on the analysis, I would implement containment measures, investigate the root cause, and work on remediation efforts. Throughout the process, I would communicate effectively with stakeholders and document all steps taken.\"\\n      }\\n    ],\\n    \"red_flags\": [\\n      \"Limited experience with at least one major cloud provider (GCP or Azure)\",\\n      \"Lack of familiarity with core security tools and concepts (Firewalls, DNS, SIEM)\",\\n      \"Inability to articulate a structured approach to handling security incidents\"\\n    ],\\n    \"confidence_score\": 0.95\\n  }\\n}'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "f422ac48c5804034bde8c0684b96ba55",
            "c26f001a9caa47f2b57ebe8d3ecc2212",
            "5b1e2009dd704902a1cbcb82872afdf8",
            "f91c4ead2e9244158e2ebcd11b25fa8b",
            "acb7bb267a71470b80833b0d6ba6f3e0",
            "b1f7ac0b06e642889f1465176e923661",
            "3a87c8be722748dfbf6b738cbcd3e469",
            "f64a935609034e9db38b48ab4f365d9c",
            "e6733dd83da547f583574a4e020fb9bc",
            "26083e99e8b34a078770c8ebc667d930",
            "2d8dcc69810a413ca5b1b11e2a916503",
            "49c46a0b6a724d3692d04a2ed239d14c",
            "1fc69db0606e4282af83defb072d05d9",
            "d528c6e335534898abb1211bab9274a2",
            "56811db0016b4116bd66971fca64126b",
            "4501dcb77dc447e6a7103e942ac903e7",
            "c5931dfe00c943e0b8c0cdfc4a655f28",
            "0a9bf37219ba4166b2739ff32680a644",
            "a30de4d9e4de44fc8b455cd7b8522616",
            "8fd12514fe7a479389d953a79674d309"
          ]
        },
        "id": "4fC0ioFCp4WO",
        "outputId": "5dd5e4c4-9f26-40f2-991a-a531eb817a2a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f422ac48c5804034bde8c0684b96ba55"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "Wz-Q8CpNp-GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "Koa6MJ2es-OY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU bitsandbytes"
      ],
      "metadata": {
        "id": "2UETLRE3sVJT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # Can select any from the below:\n",
        "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
        "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
        "    # And also all Instruct versions and Math. Coding verisons!\n",
        "    model_name = \"unsloth/Qwen2.5-7B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "e65c6e97dca34549b6c0299dbae46aee",
            "7f959efc018c41478f084f8e78613dc5",
            "a6f5d3e9b3374a11b3610f19b1afa400",
            "20259d8b13324742aea976119f0d6c60",
            "4f5b72fff2d44494b0ceefc2f07f0ed6",
            "145c28dd67ba4de3a4134ee0154afe03",
            "22864a0e53ce43f58d209319c40e26ce",
            "929ef32371d74922b9ae50595cb39d47",
            "f5e8a2c778be4c1db839729f7181111e",
            "59d5900e5e5a47c69cdf91db1153b55f",
            "a54a0fc9182d40929aca5d74a9081b3a",
            "04fb643e7d0a4d2ca984fbda466f0b5c",
            "6fd2ac05f54849b7886b48717aedeeb3",
            "813d30810ff04916a8b38b2fc6481a28",
            "e70fbcd65abd43789ffe93f0b96d96ff",
            "d0feca0de61b4c089d0006eb53c42ff9",
            "de6b099c58c24b889107d7ff964ff756",
            "9e9e54b73fdb4f0cb7a1513bf699aff7",
            "3ca621e52f68451ca48ab18e4b948709",
            "9f6f43644c6a427da27d2c97a52102fb",
            "fa620968d76643048152de5f3823d38f",
            "1fc34006835447f5b9acde34675c6626",
            "1e8930b71cc7408585be38337bb4c2e9",
            "a71f74f08d244f189cf5b7bc20081493",
            "048705f92fc548b6b1d469b44bb23751",
            "8d218c5d9a14499a9bb0a6361a73345c",
            "536c0223366945a0bae5778f2dcd28f3",
            "89616870647a4431889bc29c21fa8874",
            "15cce167383f44379234859b272ef559",
            "9d5f9a0c6e4547ae93bea460aff2f1ba",
            "a718762e77e84e82a16e0e24c9eb2ded",
            "86efe311d8644a6eb6c79ef249338b38",
            "5d0b43cd0566430591379745cfddd51d",
            "6afea39aa7b64685a8bb576941a16882",
            "cd7b7063fd57421194c98d6911559bb3",
            "e009fc800730402f8793b08c5e906852",
            "5cacc89209454ec686bffd6ada853679",
            "b75bfeb08a214d72b46cebe68f1759ca",
            "e6f69f17d83d48a69b0aad8c42c6efde",
            "96c149c137a1474e9d605c902b1840d6",
            "befa0054699e4876a937eb0bd4608832",
            "811cc2bb59cf4ac8873f7caad84ce80b",
            "f4634b3117ab470c8f98638fd09be998",
            "8286e948c22d4766a333bc4db94cfcf4",
            "e2f735bb643f4b189258507bbf1b634b",
            "9e2f2f72b7324a1688e0135ae4529bfe",
            "d6e8f68745b6411a9028a378b266105b",
            "84d64b5b770f431c8e45a77ce5020863",
            "feecd50e97e54c6784a84d33cbbaff0f",
            "e6e27451d1d64fee8fd4f90b79615720",
            "639460fa96be4ff4864028e15ce8af1a",
            "1bf13a67f4c4420088a553466354adfe",
            "df0ea01b88b9418e84fe36d876342868",
            "24b0b80f7cf54b7b8f333cc1768f2469",
            "12ce643124d94ab48276d97457a53674",
            "1aa743b5cf5e4ba394105054b9673ea6",
            "64daa2f90ac94f2c86c8147938f09de7",
            "6a3077e43e854228b0dbb24fbef75ffc",
            "b25b5f46e5c0411dae685063bbd2b132",
            "8e14818f520b4066a443b097a34b4dcd",
            "ae44d18e6fb2496f9b98d21af1343224",
            "279da7e600fd4ddeb0237efeeacee0b8",
            "97aefbbd3eb447a897c03f1187c49c68",
            "baa1c39aba1f4fdaa632eddee971fcd9",
            "8985fed66280412498b292b33d5bf02c",
            "22550a1eeedc467db08067df6416a60a",
            "78eff48cabb8476b8bdbb62bd12272cc",
            "b6d88883cdae45099fed74c922bdd978",
            "d46a1429bdea45fe9d2fdd8dae9eff12",
            "5655bc45c2e24197a0b23388cdba421d",
            "1e2218c6afe94770a0741292596e0dc3",
            "1889feac5df64f01b5fb08d68fe80e12",
            "45cc3440a0d74096bb8fdf3960071cbe",
            "a409f455a40a481c8aee32040605f828",
            "b890e28e93ea43a1ae81ced41e63aae6",
            "88e8569695644a02bafc0672d7a92e24",
            "475def9a848e4df7a4aa72e9ff3f9740",
            "7ef4f087acd149d7ac1a151c852c0197",
            "fbbeaeabb73643bd8fbf8095c811c62b",
            "6ff9fe29d3da40aa87d4594c41a91644",
            "7cba8170897b473486eefcd0b037dac5",
            "dc2efd6e00a240ca8a1ee3ea4ae64cba",
            "e13c8828e18c4b8f9281d46967a59561",
            "a83da4f123494bef812083cd814ebcac",
            "ad91ebff8da34cd69c169ba295bfa93c",
            "89ad5f7021a849a49dce8d6993936634",
            "403d7622e8624a9091f58505a697d48b",
            "b719e480ebdc42559fdfb3d931ba3ba0",
            "dba95985c69448ad88c515383a1ae4c2",
            "161187bb55f14a94afd7e33696ba12fa",
            "d8bc9eeb14b544498cdc5e5b9ea72842",
            "84d8fd52040442638abdd6034312c505",
            "adb975ddde614396958a778909b4ca20",
            "4e12f681239849c59ae35bb5a590203d",
            "249d03815af74532948a0167d1e949dc",
            "f4aaa3b4fb3e4b3c89833119b0433b7f",
            "42143dc27d96459182611d6c2e50b07a",
            "50529c723cda4c6598aa57a475d79251",
            "63b2fba8564046d0803e9494559bf255",
            "8a5682406b5a448c833e63c925ad0fff",
            "acbe4eac52814abdb0a587b58d3abe45",
            "9f2e3167358747ce97c79e1c1fa35884",
            "781c4d09f68547369ec90ecb52103857",
            "79e3244de69e450f8e2b0c1ad0370bf2",
            "178bb7f96e9141b4a7c3103e99387529",
            "b02038a144274f858901de52aef49117",
            "15125595990e4cfb81b5730f04e5e1cb",
            "28ef3fe19be947c9ab605a196058ff0f",
            "96d5c88b41954fa0b09e89e94b189b58",
            "458354f0cabe448b89aff79110ec5efa",
            "dd187e09b66a4bb4ae235fd5898b00c3",
            "05150fb6ed6549d790ec46bda11e4c7c",
            "fa2f88506bd74019b897dff00308f107",
            "5c5ebacb93214174a2003a412512c36c",
            "8dacd2a6da484b819b040074ff87c02d",
            "d271c2075352456bb9562d30b26388e7",
            "9fe79ed88ed7420b88e07aeceabaea6a",
            "3b2e1f7df5094503bd6dc7fffd809843",
            "ea9704a5510841c0aabd638d1e9552df",
            "92eb3dc3a1eb4e16b9d078aa89c446ed",
            "83148a2f9b4140e9b53c413c7b10f5f3",
            "381cbf4c8c83409ba8617f1770f4e6d0",
            "9c90a0dbe890456dbf01f4f75716db87",
            "7cc10af5c2c5451b857d395f9e752ace",
            "e03570f083f34484ad9f029c94b6a589",
            "ee4c490d60074e4c8ece33cc1554a37f",
            "91365aaf357b433283b24004eb0d0da0",
            "1d00f814a9ea4c5aba1630069c56ca09",
            "a28347865b1349299cec34d885d3a82b",
            "52580694ef854ea09f0e5653fd333e73",
            "a5f4993dc3a84492a91e6fb56ebb27bd",
            "b70bffdece3240c1b9ae7e928e5ea0ff"
          ]
        },
        "id": "hRSn88FQrqt6",
        "outputId": "eabae408-bada-4490-e518-f714674b7d7c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-e8ea81163a91>:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/106k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e65c6e97dca34549b6c0299dbae46aee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04fb643e7d0a4d2ca984fbda466f0b5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e8930b71cc7408585be38337bb4c2e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6afea39aa7b64685a8bb576941a16882"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2f735bb643f4b189258507bbf1b634b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1aa743b5cf5e4ba394105054b9673ea6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78eff48cabb8476b8bdbb62bd12272cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ef4f087acd149d7ac1a151c852c0197"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dba95985c69448ad88c515383a1ae4c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a5682406b5a448c833e63c925ad0fff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd187e09b66a4bb4ae235fd5898b00c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "381cbf4c8c83409ba8617f1770f4e6d0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IM-GWcZrqiP",
        "outputId": "eb5468d0-6b08-4669-cc0b-d1a94032c2f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\": texts }\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "hp9Lwm9Bxrds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "075d8e30d7ab4560bd125eac493a5402",
            "d80f2449acae4c0697ccc3587676fbf4",
            "afb38a5f4db54400ad20ea11763a2d2c",
            "c78ec98a624b4978b41d58f6e2eedb9b",
            "8822f4d16e6d4efd822e1a2301a2de65",
            "8535b3419394496f8af78e464191b513",
            "ae888c53e2604e5fb57c968dbb1fe529",
            "b993aa1666e74533a7c347f31fb620d5",
            "09079dbf45f147dcbb8e02e0dcfa3480",
            "d4fba24ba1c24a6c8fefab07b47107a3",
            "28ffe115235d400cb1e54db2cf7216b2"
          ]
        },
        "outputId": "c25dc049-8f37-4156-87db-9a046b7c27b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/112 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "075d8e30d7ab4560bd125eac493a5402"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "2iIQm28QqoTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74183070-cd2d-4c05-dbe4-a0a805cd18e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'text'],\n",
              "    num_rows: 112\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 2,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 14,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8e9494d396c54abb84f838b17f27f372",
            "2def28b490744854b1bd927033cb2ca2",
            "16c92f0f69d14a23b7e9cc24e5daeac2",
            "4dfd441503044da5aad421f92f5ee95b",
            "d0689809955e4c00b1b5c92d9e9c5c1f",
            "6152710351054d20bb2f449b1b665ecb",
            "14f133f3445d4013a2d8f0f70472c706",
            "997036dfeb2c4922891fcf405f7d7eb4",
            "cd171a0b46d246718f45954e647527a0",
            "6e3b0c0625524eb88f95d9010af859ad",
            "40f5468ef6e6462e985dff8e7683bca8"
          ]
        },
        "id": "sUAHD-Sat4Vs",
        "outputId": "61f25a05-9409-4a19-b4de-8284c8172e6e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/112 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e9494d396c54abb84f838b17f27f372"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDbv_mkRvWod",
        "outputId": "864625af-52ef-4515-8ac2-f01ecd242350"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "9.652 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ATzlEuTsvh55",
        "outputId": "9620a3ac-2196-4bc6-a493-c35ee34b031c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 112 | Num Epochs = 1 | Total steps = 14\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 40,370,176/7,000,000,000 (0.58% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/14 08:47, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.088700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tyd2Rehvij3",
        "outputId": "71021de3-3eaa-4898-ad31-b4d6076f3f82"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "568.0398 seconds used for training.\n",
            "9.47 minutes used for training.\n",
            "Peak reserved memory = 9.652 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 65.477 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "OSB4UhPU0Isd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\\n\".join([\n",
        "    \"You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"The Output Structure must follow the provided pydantic details.\",\n",
        "    \"# description:\",\n",
        "        data[0]['description'],\n",
        "        \"\",\n",
        "        \"# Task:\",\n",
        "        data[0]['task'],\n",
        "        \"\",\n",
        "        \"# Output Scheme:\",\n",
        "        data[0][\"output_scheme\"],\n",
        "        \"\"\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "kO7TLvDhrqft"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ULH8PtiL2Vtv",
        "outputId": "51a5d800-1a29-4d72-99b7-2a9bacdee77c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\\nFollow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\\nThe Output Structure must follow the provided pydantic details.\\n# description:\\nBenefits:\\n\\nBonus based on performance\\nCompetitive salary\\nHome office stipend\\nPaid time off\\nTraining & development\\n\\n\\nWe are seeking a DevOps Engineer to join our growing team and take ownership of our cloud infrastructure, CI/CD pipelines, system reliability, observability, and scalability. The ideal candidate is passionate about automation, cloud technologies, and security best practices, ensuring seamless deployment and high availability of our applications.\\n\\nResponsibilities\\n\\nDesign, implement, and manage CI/CD pipelines to streamline software development and deployment.\\nMaintain and optimize cloud infrastructure (AWS, Azure) to ensure scalability, security, and cost-effectiveness.\\nAutomate infrastructure provisioning, monitoring, and management using Infrastructure as Code (IaC) tools (Terraform, Ansible, etc.).\\nMonitor system performance, troubleshoot issues, and ensure high availability and reliability.\\nCollaborate with software engineers to enhance deployment strategies and improve development workflows.\\nImplement security best practices to safeguard infrastructure and applications.\\nManage containerization and orchestration tools like Docker and AWS ECS.\\nOptimize logging, monitoring, and alerting systems (ELK stack, etc.).\\nStay up to date with the latest DevOps trends, tools, and best practices.\\n\\n\\nRequirements\\n\\n3+ years of experience in a DevOps or Site Reliability Engineering (SRE) role.\\nStrong proficiency in cloud platforms (AWS, GCP, Azure) and cloud-native services.\\nExperience with CI/CD tools (GitHub Actions, GitLab CI/CD, etc.).\\nProficiency in Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Ansible.\\nStrong scripting skills in Bash, Python for automation.\\nHands-on experience with Docker and AWS ECS for container orchestration.\\nKnowledge of monitoring and logging tools (ELK stack, Datadog, etc.).\\nExperience with database management and performance optimization (SQL, NoSQL).\\nUnderstanding of security best practices, networking, and system administration.\\nExperience working in an agile startup environment is a plus.\\n\\n\\nNice-to-Have Skills\\n\\nDevelopment experience with Django\\nHands-on experience with Kubernetes for container orchestration.\\nExposure to AI/ML workloads.\\n\\n\\nWho we are\\n\\nRemarcable is a cloud based platform that helps electrical contractors and distributors streamline purchasing processes to save time and money.\\n\\nDedicated to the Electrical Contractor Industry, Remarcable provides cloud-based Procurement & Tool Management Software nationwide. With multiple workflows, two applications in one software, and direct contractor accounting integrations, Remarcable significantly, and efficiently, increases communication, streamlines workflows, and provides visibility for all users.\\n\\nOur team is composed of contractor and distribution experts located coast to coast. Through collaborations with industry leaders, we\\'ve gained insight into the struggles they face. Together, we believe in providing a solution that brings efficiency, visibility, and better communication to streamline the relationship between the contractor and distributor partners.\\n\\nOur Mission\\n\\nTo advance the adoption of technology in the construction industry and bring better efficiency, visibility, and communication to our customers.\\n\\nSalary Range Disclaimer\\n\\nThe base salary range represents the low and high end of the Remarcable salary range for this position. Actual salaries will vary depending on factors including but not limited to location, experience, and performance. The range listed is just one component of Remarcable‚Äôs total compensation package for employees. Other components may include: PTO and a Bonus plan.\\n\\nWork remote temporarily due to COVID-19.\\n\\n# Task:\\nExtrat the job description details into a JSON.\\n\\n# Output Scheme:\\n{\"$defs\": {\"key_terms\": {\"properties\": {\"term\": {\"description\": \"Technical Term or jargon from the job description\", \"title\": \"Term\", \"type\": \"string\"}, \"explanation\": {\"description\": \"A simple explanation of the term in the context of the role. This helps recruiters understand technical jargon without needing domain expertise.\", \"title\": \"Explanation\", \"type\": \"string\"}}, \"required\": [\"term\", \"explanation\"], \"title\": \"key_terms\", \"type\": \"object\"}, \"proposed_screening_questions_with_answers\": {\"properties\": {\"question\": {\"description\": \"A role-specific question to assess candidate expertise. The question should focus on technical skills, problem-solving, or past experiences relevant to the role.\", \"title\": \"Question\", \"type\": \"string\"}, \"example_answer\": {\"description\": \"An example of a strong candidate response to the question. The answer should demonstrate technical depth, problem-solving, and relevance to the role.\", \"title\": \"Example Answer\", \"type\": \"string\"}}, \"required\": [\"question\", \"example_answer\"], \"title\": \"proposed_screening_questions_with_answers\", \"type\": \"object\"}, \"skill_priorities\": {\"properties\": {\"must_have\": {\"description\": \"List of essential skills required for the role. These are non-negotiable and should be explicitly mentioned in the job description.\", \"items\": {\"type\": \"string\"}, \"title\": \"Must Have\", \"type\": \"array\"}, \"nice_to_have\": {\"description\": \"List of preferred skills that are beneficial but not mandatory. These are often marked as \\'preferred,\\' \\'a plus,\\' or \\'optional\\' in the job description.\", \"items\": {\"type\": \"string\"}, \"title\": \"Nice To Have\", \"type\": \"array\"}}, \"required\": [\"must_have\", \"nice_to_have\"], \"title\": \"skill_priorities\", \"type\": \"object\"}}, \"properties\": {\"role_summary\": {\"title\": \"Role Summary\", \"type\": \"string\"}, \"key_terms\": {\"items\": {\"$ref\": \"#/$defs/key_terms\"}, \"title\": \"Key Terms\", \"type\": \"array\"}, \"skill_priorities\": {\"$ref\": \"#/$defs/skill_priorities\"}, \"proposed_screening_questions_with_answers\": {\"items\": {\"$ref\": \"#/$defs/proposed_screening_questions_with_answers\"}, \"title\": \"Proposed Screening Questions With Answers\", \"type\": \"array\"}, \"red_flags\": {\"items\": {\"type\": \"string\"}, \"title\": \"Red Flags\", \"type\": \"array\"}, \"confidence_score\": {\"title\": \"Confidence Score\", \"type\": \"number\"}}, \"required\": [\"role_summary\", \"key_terms\", \"skill_priorities\", \"proposed_screening_questions_with_answers\", \"red_flags\", \"confidence_score\"], \"title\": \"JobAnalysis\", \"type\": \"object\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7vnjd1Bt5Rdp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction, # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "match = re.search(r\"### Response:\\s*(\\{.*)\", decoded, re.DOTALL)\n",
        "if match:\n",
        "    clean_output = match.group(1).strip()\n",
        "else:\n",
        "    clean_output = decoded.strip()\n",
        "\n",
        "print(clean_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpWOtRTr0obC",
        "outputId": "bcef4456-12dd-4ddc-f361-eaffbf2a565f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"JobAnalysis\": {\n",
            "    \"role_summary\": \"DevOps Engineer to join our growing team and take ownership of cloud infrastructure, CI/CD pipelines, system reliability, observability, and scalability.\",\n",
            "    \"key_terms\": [\n",
            "      {\n",
            "        \"term\": \"CI/CD\",\n",
            "        \"explanation\": \"Continuous Integration/Continuous Deployment, a set of practices for automating the build, test, and deployment of software.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"IaC\",\n",
            "        \"explanation\": \"Infrastructure as Code, a method of managing and provisioning infrastructure using code.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"ELK stack\",\n",
            "        \"explanation\": \"Elasticsearch, Logstash, and Kibana, a popular open-source platform for log management and analysis.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"Terraform\",\n",
            "        \"explanation\": \"An infrastructure as code tool for provisioning, configuring, and managing infrastructure.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"Ansible\",\n",
            "        \"explanation\": \"An open-source automation tool for configuring and managing infrastructure.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"Docker\",\n",
            "        \"explanation\": \"An open-source platform for building, shipping, and running containerized applications.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"AWS ECS\",\n",
            "        \"explanation\": \"Amazon Elastic Container Service, a managed container orchestration service.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"Kubernetes\",\n",
            "        \"explanation\": \"An open-source container orchestration platform for automating deployment, scaling, and management of containerized applications.\"\n",
            "      }\n",
            "    ],\n",
            "    \"skill_priorities\": {\n",
            "      \"must_have\": [\n",
            "        \"Cloud platforms (AWS, GCP, Azure)\",\n",
            "        \"CI/CD tools (GitHub Actions, GitLab CI/CD, etc.)\",\n",
            "        \"Infrastructure as Code (IaC) tools (Terraform, CloudFormation, or Ansible)\",\n",
            "        \"Scripting skills in Bash, Python for automation\",\n",
            "        \"Docker and AWS ECS for container orchestration\",\n",
            "        \"Monitoring and logging tools (ELK stack, Datadog, etc.)\",\n",
            "        \"Database management and performance optimization (SQL, NoSQL)\"\n",
            "      ],\n",
            "      \"nice_to_have\": [\n",
            "        \"Development experience with Django\",\n",
            "        \"Hands-on experience with Kubernetes for container orchestration\",\n",
            "        \"Exposure to AI/ML workloads\"\n",
            "      ]\n",
            "    },\n",
            "    \"proposed_screening_questions_with_answers\": [\n",
            "      {\n",
            "        \"question\": \"Describe your experience with CI/CD pipelines and how you have automated the build, test, and deployment process.\",\n",
            "        \"example_answer\": \"I have experience with multiple CI/CD tools such as GitHub Actions and GitLab CI/CD. I have automated the build, test, and deployment process by creating workflows that trigger on code commits, run automated tests, and deploy the application to a staging environment. I have also implemented feature flags to control the rollout of new features and rollbacks in case of issues.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"How do you ensure the security of cloud infrastructure and applications?\",\n",
            "        \"example_answer\": \"I follow security best practices such as using secure authentication methods, implementing network segmentation, and regularly updating software and security patches. I also use tools like AWS WAF and Cloudflare to protect against common web attacks. I have experience implementing security policies and procedures, conducting security audits, and responding to security incidents.\"\n",
            "      }\n",
            "    ],\n",
            "    \"red_flags\": [\n",
            "      \"Lack of experience with cloud platforms or CI/CD tools\",\n",
            "      \"Inability to explain technical jargon or concepts\",\n",
            "      \"Lack of experience with infrastructure as code (IaC) tools\",\n",
            "      \"Inability to demonstrate automation skills\",\n",
            "      \"Lack of experience with monitoring and logging tools\",\n",
            "      \"Inability to explain security best practices\"\n",
            "    ],\n",
            "    \"confidence_score\": 0.95\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_desc_exp = open(\"/content/job_desc.txt\", \"r\")\n",
        "job_desc_exp_from_the_web = job_desc_exp.read()"
      ],
      "metadata": {
        "id": "nuYe6Snt9k5M"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_desc_exp_from_the_web"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "sLUiV0q0DtoO",
        "outputId": "fbfda6c9-78d2-440c-8d5b-67366f834619"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reports to: Director of Marketing Analytics\\nAbout Nexora:\\nNexora is a fast-growing technology company revolutionizing the way businesses engage with customers through AI-powered marketing solutions. Our flagship platform, NexoraPulse, enables brands to personalize campaigns at scale, harnessing advanced analytics and automation. We are dedicated to fostering innovation, collaboration, and customer-centricity across all areas of our business.\\n\\nAt Nexora, we believe in building a workplace where curiosity is celebrated, impact is rewarded, and every team member is empowered to thrive. Our values‚Äîownership, agility, inclusivity, and innovation‚Äîguide us in everything we do.\\n\\nJob Overview:\\nWe are seeking a skilled and proactive Marketing Data Analyst to join our Analytics team. In this role, you will be responsible for collecting, interpreting, and visualizing marketing performance data to drive strategic decisions. You will work closely with marketing, product, and sales teams to provide actionable insights and improve ROI on marketing campaigns. Your work will help us better understand user behavior, optimize our marketing funnel, and inform key growth initiatives.\\n\\nJob Responsibilities:\\n\\nData Collection & Integration: Gather data from CRM systems, email platforms, Google Analytics, social media, and third-party tools\\n\\nCampaign Analysis: Monitor and evaluate performance of marketing campaigns across channels (email, paid media, SEO, etc.)\\n\\nData Cleaning & Transformation: Ensure accuracy, consistency, and completeness of marketing data\\n\\nInsight Generation: Use statistical models to identify trends, predict outcomes, and provide actionable insights\\n\\nDashboard Creation: Build and maintain dashboards in Tableau or Looker for tracking KPIs and campaign performance\\n\\nCross-Functional Collaboration: Work closely with marketing managers, product teams, and stakeholders to align on goals and reporting needs\\n\\nA/B Testing: Design and analyze experiments to improve marketing strategies and conversion rates\\n\\nReporting: Present clear, concise findings and recommendations to both technical and non-technical audiences\\n\\nRequired Experience, Qualifications, and Skills:\\n\\n2+ years of experience in marketing analytics or data analysis roles\\n\\nProficiency in SQL and Excel for data manipulation and reporting\\n\\nExcellent problem-solving and critical thinking skills\\n\\nPreferred Experience, Qualifications, and Skills:\\n\\nExperience with BI tools such as Tableau, Power BI, or Looker\\n\\nFamiliarity with Python or R for advanced analytics\\n\\nEqual Opportunity:\\nNexora is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We welcome candidates of all backgrounds and experiences to apply.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_001 = \"\\n\".join([\n",
        "    \"You are an AI assistant that converts job descriptions into structured JSON data to assist recruiters.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"The Output Structure must follow the provided pydantic details.\",\n",
        "    \"# description:\",\n",
        "        job_desc_exp_from_the_web,\n",
        "        \"\",\n",
        "        \"# Task:\",\n",
        "        data[0]['task'],\n",
        "        \"\",\n",
        "        \"# Output Scheme:\",\n",
        "        data[0][\"output_scheme\"],\n",
        "        \"\"\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "3Y-ZSZhM9e5N"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction_001, # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1512, use_cache = True)\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "match = re.search(r\"### Response:\\s*(\\{.*)\", decoded, re.DOTALL)\n",
        "if match:\n",
        "    clean_output = match.group(1).strip()\n",
        "else:\n",
        "    clean_output = decoded.strip()\n",
        "\n",
        "print(clean_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rUeDW-s-Cer",
        "outputId": "2d025c72-6537-41dd-eb5a-3dd1431428a1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"JobAnalysis\": {\n",
            "    \"role_summary\": \"The Marketing Data Analyst will be responsible for collecting, interpreting, and visualizing marketing performance data to drive strategic decisions. They will work closely with marketing, product, and sales teams to provide actionable insights and improve ROI on marketing campaigns.\",\n",
            "    \"key_terms\": [\n",
            "      {\n",
            "        \"term\": \"CRM\",\n",
            "        \"explanation\": \"Customer Relationship Management system used to manage customer interactions and data.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"BI\",\n",
            "        \"explanation\": \"Business Intelligence tools used for data analysis and reporting.\"\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"A/B Testing\",\n",
            "        \"explanation\": \"A method of comparing two versions of a marketing campaign to determine which performs better.\"\n",
            "      }\n",
            "    ],\n",
            "    \"skill_priorities\": {\n",
            "      \"must_have\": [\n",
            "        \"SQL\",\n",
            "        \"Excel\",\n",
            "        \"Tableau\",\n",
            "        \"Python/R\"\n",
            "      ],\n",
            "      \"nice_to_have\": [\n",
            "        \"Google Analytics\",\n",
            "        \"Power BI\",\n",
            "        \"Looker\"\n",
            "      ]\n",
            "    },\n",
            "    \"proposed_screening_questions_with_answers\": [\n",
            "      {\n",
            "        \"question\": \"Describe your experience with SQL and how you have used it to analyze marketing data.\",\n",
            "        \"example_answer\": \"I have 3 years of experience using SQL to extract and manipulate data from various sources. For example, I used SQL to analyze customer behavior on our website and identify patterns in their purchasing habits. This helped us optimize our marketing campaigns and improve conversion rates.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"How would you design an A/B test to compare the performance of two email campaigns?\",\n",
            "        \"example_answer\": \"To design an A/B test, I would first define the hypothesis and key performance indicators (KPIs) to measure. Then, I would randomly assign recipients to either the control group or the test group. Next, I would send out the two email campaigns and track the KPIs for each group. Finally, I would analyze the results to determine which campaign performed better and why.\"\n",
            "      }\n",
            "    ],\n",
            "    \"red_flags\": [\n",
            "      \"Lack of experience with SQL or Excel\",\n",
            "      \"Inability to explain technical concepts in simple terms\",\n",
            "      \"Lack of experience with BI tools\"\n",
            "    ],\n",
            "    \"confidence_score\": 0.9\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get userdata\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_WRITE_TOKEN')\n"
      ],
      "metadata": {
        "id": "9pDXvg5T6Kdc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"mahdihammi/jobs_Qwen_lora_model\", token = HF_TOKEN) # Online saving\n",
        "tokenizer.push_to_hub(\"mahdihammi/jobs_Qwen_lora_model\", token = HF_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "4e7974b1f61d4a3dbef3cdf1a8afde99",
            "5bfee1e27c694d2da156363805d11438",
            "8474c41f07c24c6698861e5bccaff7fd",
            "76e92848229e4393b974cd4ccd748bc7",
            "cb9b57f91d4444e89f94e65c1d0abd93",
            "9b223f9992bb43439e9c5c3e6ccd35e0",
            "958ab5373d6344f0afc5332a375ff78a",
            "40f9b3d6d3c144558cc4ab080e8fb829",
            "b48235707dcb407086561e9927684a31",
            "187fbb6cafa143b28f8c321519301fad",
            "16a62d5f852f41dfa34db5a54887a90d",
            "dd28ddb5161c4634a4d94f7231858815",
            "114380066ece43cbbfd84345069fb87a",
            "0077c72c542f471480af510f943d7940",
            "7631141a74b444b9801e4ce1bc6b3a7a",
            "51c90b402aa749d3a4b72479ca3e62d6",
            "478803523f32488a9427bb9af2a75154",
            "f562e0f3bbd047d8a259dc7ff7ac7f27",
            "f06539fc13f24b82ae692f3dd5babc4b",
            "323efa5bfaf24caa8bad4e37721c5c9b",
            "252730e7ce2046eab383461950a90160",
            "56e43a005f1e4f16b3ddb6cf861f1e4e",
            "a8a8bca651ad4c7387725d52a49a984e",
            "b894c391c2fb4f09a81dbd0ef6520ad4",
            "6c4f5f6423134d60a1fd3d9cbf0a007f",
            "b44b2e01c1cb427fa1d98fe9c623f9df",
            "c6667b8f6d3d4032a47f9ecb0d3e278d",
            "c98eeffa05d743fe940b79e47a59c2f9",
            "4c457dfe309d4d0eb93583936e834662",
            "4a97803a8c564828ad4af8cfaacc02e0",
            "6c7c66aefaef4544992e42dcb33bd6d5",
            "6b0c7eab7983417daba71cd269ffa25c",
            "3660d8066dd74de087af192917d827fe"
          ]
        },
        "id": "5S2eMRth548D",
        "outputId": "017c505a-4bd2-4e8c-cf59-597ca66294cb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/593 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e7974b1f61d4a3dbef3cdf1a8afde99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/162M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd28ddb5161c4634a4d94f7231858815"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to https://huggingface.co/mahdihammi/jobs_Qwen_lora_model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8a8bca651ad4c7387725d52a49a984e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pushed model\n"
      ],
      "metadata": {
        "id": "2462SnAb73zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"mahdihammi/jobs_Qwen_lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction, # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1028)"
      ],
      "metadata": {
        "id": "bexuUZcs7hTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}